# COO-LLM Documentation

ğŸš€ **Intelligent Load Balancer for LLM APIs with Full OpenAI Compatibility**

COO-LLM is a high-performance reverse proxy that intelligently distributes requests across multiple LLM providers (OpenAI, Google Gemini, Anthropic Claude) and API keys. It provides seamless OpenAI API compatibility, advanced load balancing algorithms, real-time cost optimization, and enterprise-grade observability.

[![Go Version](https://img.shields.io/badge/go-1.21+-blue.svg)](https://golang.org)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://docker.com)
[![License: DIB](https://img.shields.io/badge/License-DIB-black.svg)](https://devs-in-black.web.app/#license)
[![OpenAI Compatible](https://img.shields.io/badge/OpenAI-Compatible-green.svg)](https://platform.openai.com/docs)

## ğŸš€ Features

### âœ¨ Core Capabilities
- **ğŸ”„ Full OpenAI API Compatibility**: Drop-in replacement with identical request/response formats
- **ğŸŒ Multi-Provider Support**: OpenAI, Google Gemini, Anthropic Claude, and custom providers
- **ğŸ§  Intelligent Load Balancing**: Advanced algorithms (Round Robin, Least Loaded, Hybrid) with real-time optimization
- **ğŸ’¬ Conversation History**: Full support for multi-turn conversations and message history

### ğŸ’° Cost & Performance Optimization
- **ğŸ“Š Real-time Cost Tracking**: Monitor and optimize API costs across all providers
- **âš¡ Rate Limit Management**: Sliding window rate limiting with automatic key rotation
- **ğŸ“ˆ Performance Monitoring**: Track latency, success rates, token usage, and error patterns
- **ğŸ”„ Response Caching**: Configurable caching to reduce costs and improve performance

### ğŸ¢ Enterprise-Ready
- **ğŸ”Œ Extensible Architecture**: Plugin system for custom providers, storage backends, and logging
- **ğŸ“Š Production Observability**: Prometheus metrics, structured logging, and health checks
- **âš™ï¸ Configuration Management**: YAML-based configuration with environment variable support
- **ğŸ”’ Security**: API key masking, secure storage, and authentication controls

## ğŸ Quick Start

### Local Development

```bash
# Clone and build
git clone https://github.com/your-org/coo-llm.git
cd coo-llm
go build -o bin/coo-llm ./cmd/coo-llm

# Configure with environment variables
export OPENAI_API_KEY="sk-your-key"
export GEMINI_API_KEY="your-gemini-key"

# Run
./bin/coo-llm

# Test simple request
curl -X POST http://localhost:2906/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4o", "messages": [{"role": "user", "content": "Hello!"}]}'
```

### Docker

```bash
# Run with local build
docker run -p 2906:2906 \
  -e OPENAI_API_KEY="sk-your-key" \
  -e GEMINI_API_KEY="your-gemini-key" \
  -v $(pwd)/configs:/app/configs \
  khapu2906/coo-llm:latest
```

## ğŸ“š Documentation

### Quick Links
- **[Introduction](docs/Intro/Overview.md)**: Overview and architecture
- **[Configuration](docs/Guides/Configuration.md)**: Complete configuration reference
- **[API Reference](docs/Reference/API.md)**: REST API documentation
- **[Load Balancing](docs/Reference/Balancer.md)**: Load balancing algorithms and policies
- **[Deployment](docs/Guides/Deployment.md)**: Installation and production deployment
- **[LangChain Demo](langchain-demo/)**: Integration examples

### Documentation Structure
- **Intro**: Overview, architecture, and getting started
- **Guides**: User guides, configuration, and deployment
- **Reference**: Technical API, configuration, and balancer reference
- **Contributing**: Development guidelines and contribution process

## ğŸ—ï¸ Architecture

```
Client Applications (OpenAI SDK, LangChain, etc.)
    â†“ HTTP/JSON (OpenAI-compatible API)
COO-LLM Proxy
â”œâ”€â”€ ğŸº API Layer (OpenAI-compatible REST API)
â”‚   â”œâ”€â”€ Chat Completions (/v1/chat/completions)
â”‚   â”œâ”€â”€ Models (/v1/models)
â”‚   â””â”€â”€ Admin API (/admin/v1/*)
â”œâ”€â”€ âš–ï¸ Load Balancer (Intelligent Routing)
â”‚   â”œâ”€â”€ Round Robin, Least Loaded, Hybrid algorithms
â”‚   â”œâ”€â”€ Rate limiting & cost optimization
â”‚   â””â”€â”€ Real-time performance tracking
â”œâ”€â”€ ğŸ”Œ Provider Adapters
â”‚   â”œâ”€â”€ OpenAI (GPT-4, GPT-3.5)
â”‚   â”œâ”€â”€ Google Gemini (1.5 Pro, etc.)
â”‚   â”œâ”€â”€ Anthropic Claude (Opus, Sonnet)
â”‚   â””â”€â”€ Custom providers
â”œâ”€â”€ ğŸ’¾ Storage Layer
â”‚   â”œâ”€â”€ Redis (production, with clustering)
â”‚   â”œâ”€â”€ Memory (development)
â”‚   â”œâ”€â”€ File-based (simple deployments)
â”‚   â””â”€â”€ HTTP (remote storage)
â””â”€â”€ ğŸ“Š Observability
    â”œâ”€â”€ Structured logging (JSON)
    â”œâ”€â”€ Prometheus metrics
    â”œâ”€â”€ Response caching
    â””â”€â”€ Health checks
    â†“
External LLM Providers (OpenAI, Gemini, Claude APIs)
```

## ğŸ“Š Key Metrics

- **ğŸš€ Load Balancing**: Intelligent distribution across 3+ providers
- **ğŸ’° Cost Optimization**: Real-time cost tracking and automatic optimization
- **âš¡ Rate Limiting**: Sliding window rate limiting with key rotation
- **ğŸ“ˆ Performance**: Sub-millisecond routing with comprehensive monitoring
- **ğŸ”’ Security**: API key masking and secure storage
- **ğŸ“Š Observability**: Prometheus metrics, structured JSON logging

---

**COO-LLM** - The Intelligent LLM API Load Balancer ğŸš€

*Load balance your LLM API calls across multiple providers with OpenAI compatibility, real-time cost optimization, and enterprise-grade reliability.*