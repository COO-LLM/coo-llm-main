version: "1.0"

server:
  listen: ":2906"
  admin_api_key: "${ADMIN_API_KEY}"
  cors:
    enabled: true
    allowed_origins: ["*"]
    allowed_methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"]
    allowed_headers: ["*"]
    allow_credentials: true
    max_age: 86400
  webui:
    enabled: true
    admin_id: "${ADMIN_ID}"
    admin_password: "${ADMIN_PASSWORD}"
    # web_ui_path: "/path/to/custom/ui"  # Optional: path to custom web UI build directory

logging:
  file:
    enabled: true
    path: "./logs/llm.log"
    max_size_mb: 100
    max_backups: 5
  prometheus:
    enabled: true
    endpoint: "/metrics"
  providers: []

storage:
  runtime:
    type: "sql"
    addr: "./data/coo-llm.db" 

llm_providers:
  - id: "openai"
    type: "openai"
    api_keys: ["${OPENAI_API_KEY}"]
    model: "gpt-4o"
    pricing:
      input_token_cost: 0.15
      output_token_cost: 0.60
    limits:
      req_per_min: 200
      tokens_per_min: 100000
      max_tokens: 128000
      session_limit: 6000000
      session_type: "1h"
  - id: "gemini"
    type: "gemini"
    api_keys: ["${GEMINI_API_KEY}"]
    model: "gemini-2.5-pro"
    pricing:
      input_token_cost: 1.25
      output_token_cost: 10.0
    limits:
      req_per_min: 150
      tokens_per_min: 80000
      max_tokens: 1000000
      session_limit: 10000000
      session_type: "1h"
  - id: "claude"
    type: "claude"
    api_keys: ["${CLAUDE_API_KEY}"]
    base_url: "https://api.anthropic.com"
    model: "claude-opus"
    pricing:
      input_token_cost: 15.0
      output_token_cost: 75.0
    limits:
      req_per_min: 100
      tokens_per_min: 60000
      max_tokens: 200000
      session_limit: 5000000
      session_type: "1h"
  - id: "together"
    type: "together"
    api_keys: ["${TOGETHER_API_KEY}"]
    model: "meta-llama/Llama-2-70b-chat-hf"
    pricing:
      input_token_cost: 0.0000002
      output_token_cost: 0.0000002
    limits:
      req_per_min: 100
      tokens_per_min: 100000
      max_tokens: 32000
      session_limit: 2000000
      session_type: "1h"
  - id: "openrouter"
    type: "openrouter"
    api_keys: ["${OPENROUTER_API_KEY}"]
    model: "anthropic/claude-3-haiku"
    pricing:
      input_token_cost: 0.00000025
      output_token_cost: 0.00000125
    limits:
      req_per_min: 50
      tokens_per_min: 50000
      max_tokens: 200000
      session_limit: 5000000
      session_type: "1h"
  - id: "mistral"
    type: "mistral"
    api_keys: ["${MISTRAL_API_KEY}"]
    model: "mistral-large-latest"
    pricing:
      input_token_cost: 0.000002
      output_token_cost: 0.000006
    limits:
      req_per_min: 100
      tokens_per_min: 100000
      max_tokens: 64000
      session_limit: 3000000
      session_type: "1h"
  - id: "cohere"
    type: "cohere"
    api_keys: ["${COHERE_API_KEY}"]
    model: "command-r-plus"
    pricing:
      input_token_cost: 0.0000015
      output_token_cost: 0.000006
    limits:
      req_per_min: 100
      tokens_per_min: 100000
      max_tokens: 128000
      session_limit: 4000000
      session_type: "1h"
  - id: "huggingface"
    type: "huggingface"
    api_keys: ["${HUGGINGFACE_API_KEY}"]
    model: "microsoft/DialoGPT-medium"
    pricing:
      input_token_cost: 0.0000002
      output_token_cost: 0.0000008
    limits:
      req_per_min: 60
      tokens_per_min: 60000
      max_tokens: 8192
      session_limit: 1000000
      session_type: "1h"
  - id: "replicate"
    type: "replicate"
    api_keys: ["${REPLICATE_API_KEY}"]
    model: "meta/llama-2-70b-chat"
    pricing:
      input_token_cost: 0.00000065
      output_token_cost: 0.00000275
    limits:
      req_per_min: 10
      tokens_per_min: 10000
      max_tokens: 8192
      session_limit: 1000000
      session_type: "1h"
  - id: "voyage"
    type: "voyage"
    api_keys: ["${VOYAGE_API_KEY}"]
    model: "voyage-large-2"
    pricing:
      input_token_cost: 0.00000012
      output_token_cost: 0.00000012
    limits:
      req_per_min: 100
      tokens_per_min: 100000
      max_tokens: 32768
      session_limit: 2000000
      session_type: "1h"
  - id: "fireworks"
    type: "fireworks"
    api_keys: ["${FIREWORKS_API_KEY}"]
    model: "accounts/fireworks/models/llama-v3-8b-instruct"
    pricing:
      input_token_cost: 0.0000002
      output_token_cost: 0.0000008
    limits:
      req_per_min: 60
      tokens_per_min: 60000
      max_tokens: 16384
      session_limit: 2000000
      session_type: "1h"

api_keys:
  - id: "client-1"
    key: "${API_KEY}"
    allowed_providers: ["*"]  # Access all providers
    description: "Default API key for all providers"

# model_aliases removed due to YAML parsing bug with colon characters in values
# Workaround: Use model names directly (e.g., "openai:gpt-4o") or define aliases at runtime

policy:
  algorithm: "hybrid"   # "round_robin", "least_loaded", "hybrid"
  priority: "balanced"  # "balanced", "cost", "req", "token" (auto-sets weights)
  hybrid_weights:       # Auto-set based on priority, or customize
    token_ratio: 0.2
    req_ratio: 0.2
    error_score: 0.2
    latency: 0.2
    cost_ratio: 0.2
  retry:
    max_attempts: 3      # Max retry attempts
    timeout: "30s"       # Timeout per attempt
    interval: "1s"       # Interval between retries
  fallback:
    enabled: true        # Enable fallback to other providers
    max_providers: 2     # Max fallback providers to try
    # providers: ["openai", "together"]  # Optional: specific fallback providers
  cache:
    enabled: true        # Enable response caching
    ttl_seconds: 10      # Cache TTL (10 seconds)
